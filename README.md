# Overview

## This project implements a scalable data pipeline using Apache Spark, PostgreSQL, and Kubernetes. The goal is to process and transform data from PostgreSQL using Spark and store the transformed data in various formats like CSV and Parquet. The pipeline is designed to handle data efficiently in a distributed environment, leveraging the power of Spark for big data processing.

## Technologies Used
- Apache Spark: A unified analytics engine for big data processing with built-in modules for streaming, SQL, machine learning, and graph processing.
- PostgreSQL: A powerful, open-source object-relational database system.
- Kubernetes (with KIND): A system for automating the deployment, scaling, and management of containerized applications.
- Docker: A tool designed to make it easier to create, deploy, and run applications using containers.
- Python: A programming language used for writing the transformation logic and data processing.
- Pyspark: Python API for Apache Spark.
